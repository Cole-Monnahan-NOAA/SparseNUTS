---
title: "Introduction to SparseNUTS"
author: "Cole C. Monnahan"
description: >
    Learn how to get started with the basics of SparseNUTS
bibliography: refs.bib
output:
  rmarkdown::html_vignette:
  toc: true
vignette: >
  %\VignetteIndexEntry{Introduction to SparseNUTS}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

The goal of SparseNUTS is to provide a user-friendly workflow for users of TMB and RTMB who want to implement the sparse no-u-turn sampler [@monnahan2025] to draw samples from a model.


This package was originally developed inside of the [adnuts](https://github.com/Cole-Monnahan-NOAA/adnuts)
package but was split off in late 2025 to have a dedicated package
for SparseNUTS for TMB and RTMB models.  The `tmbstan` package also provides an
interface to the Stan software, but lacks the ability to
decorrelate the target distribution prior to sampling.
`SparseNUTS` provides more flexible options related to the mass
matrix.

## Differences in usage between TMB and RTMB

Both TMB and RTMB models can be used with minimal user intervention, including running parallel chains. The `sample_snuts` function will detect which package is used internally and adjust accordingly. If the user wants to use models from both packages in the same session then one needs to be unloaded, e.g., `if('TMB' %in% .packages()) detach(package:TMB)`, before the other package is loaded. 

If the RTMB model uses external functions or data sets then they must be passed through via a list in the `globals` argument so they are available to rebuild the 'obj' in the parallel R sessions. Optionally, the `model_name` can be specified in the call, otherwise your model will be labeled "RTMB" in the output. TMB models do not require a globals input and the model name is pulled from the DLL name, but can be overridden if desired.

## Comparison to tmbstan 

The related package 'tmbstan' [@monnahan2018] also allows users
to link TMB models to the Stan algorithms. 'tmbstan' links
through the package 'rstan', while 'SparseNUTS' modifies the
objective and gradient functions and then passes those to
'cmdstan' through the 'StanEstimators' R package interface.  For
models without large correlations or scale differences, `tmbstan`
is likely to be faster than 'SparseNUTS' due to lower overhead and
may be a better option. Eventually, Stan may add SNUTS
functionality and an interface to 'tmbstan' developed, and in
that case `tmbstan` may be a better long term option. For TMB
users now, SNUTS via `SparseNUTS` is likely to be the best overall
package for Bayesian inference.

## SNUTS for TMB models from existing packages (sdmTMB, glmmTMB, etc.)

`SparseNUTS` works for custom TMB and RTMB models developed locally, but also for those that come in packages. Most packages will return the TMB 'obj' which can then be passed into `sample_snuts`.

For instance the `glmmTMB` package can be run like this:

```{r glmmTMB, eval=FALSE}
library(glmmTMB)
library(SparseNUTS)
data(Salamanders)
obj <- glmmTMB(count~spp * mined + (1|site), Salamanders, family="nbinom2")$obj
fit <- sample_snuts(obj)
```

## Basic usage

The recommended usage for TMB users is to let the `sample_snuts` function automatically detect the metric to use and the length of warmup period, especially for pilot runs during model development.

I demonstrate basic usage using a very simple RTMB version of the eight schools model that has been examined extensively in the Bayesian literature. The first step is to build the TMB object 'obj' that incorporates priors and Jacobians for parameter transformations. Note that the R function returns the negative un-normalized log-posterior density.

```{r schools-obj}
library(RTMB)
library(SparseNUTS)
dat <- list(y=c(28,  8, -3,  7, -1,  1, 18, 12),
            sigma=c(15, 10, 16, 11,  9, 11, 10, 18))
pars <- list(mu=0, logtau=0, eta=rep(1,8))
f <- function(pars){
  getAll(dat, pars)
  theta <- mu + exp(logtau) * eta;
  lp <- sum(dnorm(eta, 0,1, log=TRUE))+ # prior
    sum(dnorm(y,theta,sigma,log=TRUE))+ #likelihood
    logtau                          # jacobian
  REPORT(theta)
  return(-lp)
}
obj <- MakeADFun(func=f, parameters=pars,
                 random="eta", silent=TRUE)
```


### Posterior sampling with SNUTS

The most common task is to draw samples from the posterior density defined by this model. This is done with the `sample_snuts` function as follows:

```{r schools-integrate}
fit <- sample_snuts(obj, refresh=0, seed=1,
                    model_name = 'schools',
                    cores=1, chains=1,
                    globals=list(dat=dat))

```

The returned object `fit` (an object of 'adfit' S3 class) contains the posterior samples and other relevant information for a Bayesian analysis.

Here a 'diag' (diagonal) metric is selected and a very short warmup period of 150 iterations is used, with mass matrix adaptation in Stan disabled. See below for more details on mass matrix adaptation within Stan.

Notice that no optimization was done before calling `sample_snuts`. When the model has already been optimized, you can skip that by setting `skip_optimization=TRUE`, and even pass in $Q$ and $\Sigma=Q^{-1}$ via arguments `Q` and `Qinv` to bypass this step and save some run time. This may also be required if the model optimization routine internal to `sample_snuts` is insufficient. In that case, the user should optimize prior to SNUTS sampling. The returned fitted object contains a slot called `mle` (for maximum likelihood estimates) which has the conditional mode ('est'), the marginal standard errors 'se', a joint correlation matrix ('cor'), and the sparse precision matrix $Q$.

```{r}
str(fit$mle)
```

### Diagnostics

The common MCMC diagnostics potential scale reduction (Rhat) and minimum ESS, as well as the NUTS divergences (see [diagnostics section](https://mc-stan.org/docs/reference-manual/analysis.html) of the rstan manual), are printed to console by default or can be accessed in more depth via the `monitor` slot:

```{r diagnostics-schools}
print(fit)

fit$monitor |> str()
```

A specialized `pairs` plotting function is available (formally called `pairs_admb`) to examine pair-wise behavior of the posteriors. This can be useful to help diagnose particularly slow mixing parameters. This function also displays the conditional mode (point) and 95% bivariate confidence region (ellipses) as calculated from the approximate covariance matrix $\Sigma=Q^{-1}$. The parameters to show can be specified either vie a character vector like `pars=c('mu', 'logtau', 'eta[1]')` or an integer vector like `pars=1:3`, and when using the latter the parameters can be ordered by slowest mixing ('slow'), fastest mixing ('fast') or by the largest discrepancies in the approximate marginal variance from $Q$ and the posterior samples ('mismatch'). NUTS divergences are shown as green points. See help and further information at `?pairs.adfit`.

```{r pairs-schools}
pairs(fit, order='slow')


```

In some cases it is useful to diagnose the NUTS behavior by examining the "sampler parameters", which contain information about the individual NUTS trajectories.

```{r sp-schools}
extract_sampler_params(fit) |> str()
## or plot them directly
plot_sampler_params(fit)
```

The ShinyStan tool is also available and provides a convenient, interactive way to check diagnostics via the function `launch_shinytmb()`, but also explore estimates and other important quantities. This is a valuable tool for a workflow with 'SparseNUTS'.

### Bayesian inference

After checking for signs of non-convergence the results can be used for inference. Posterior samples for parameters can be extracted and examined in R by casting the fitted object to an R data.frame. These posterior samples can then be put back into the TMB object `obj$report()` function to extract any desired "generated quantity" in Stan terminology. Below is a demonstration of how to do this for the quantity theta (a vector of length 8).

```{r inference-schools}
post <- as.data.frame(fit)
post |> str()
## now get a generated quantity, here theta which is a vector of
## length 8 so becomes a matrix of posterior samples
theta <- apply(post,1, \(x) obj$report(x)$theta) |> t()
theta |> str()
```

Likewise, marginal distributions can be explored visually and compared to the approximate estimate from the conditional mode and $\Sigma$ (red lines):

```{r marginals-schools}
plot_marginals(fit)
```

## References
